{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python语言与经济大数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5_1讲 网络爬虫入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#郭峰（上海财经大学投资系副教授）\n",
    "#Email：guo.feng@mail.sufe.edu.cn\n",
    "#2019-01-22\n",
    "#本讲目录\n",
    "#5.1.1 网页构造和爬虫原理\n",
    "#5.1.2 网页的获取与阅读\n",
    "#5.1.3 简单爬虫实例\n",
    "#5.1.4 正则表达式与网页源代码解析\n",
    "#5.1.5 更多爬虫实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1.1 网页构造和爬虫原理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生活生产的网络化，导致了大量之前没有留存、没法分析的资料都数据化，这是我理解的大数据最重要特征\n",
    "#WEB数据展示特点：主要为人观看服务\n",
    "#数据背后夹杂着大量的无用信息\n",
    "#没有标准的格式\n",
    "#网络爬虫的目的就是从后台入手，剔除杂乱无用信息，从网络中提取有用信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#网络连接原理：计算机一次Request请求和服务器一次Response回应，即实现网络连接\n",
    "#爬虫原理：模拟计算机对服务器发起Request请求；接收服务器端的Response内容并解析、提取所需信息。\n",
    "#当然，往往不是采集单独一个网页，所以需要循环访问一系列网页。\n",
    "#爬虫需要安装模块，pip3 install beautifulsoup4 （在PyCharm集成环境中的安装方式参阅罗攀和蒋仟（2017，p27）\n",
    "#导入BeautifulSoup库的写法为：from bs4 import BeautifulSoup （bs4就是最新版的beautifulSoup）\n",
    "#也可以只用python 3自带的urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HTIML文档基本概念\n",
    "#元素（标签）\n",
    "#属性\n",
    "#内容，空内容\n",
    "#容器\n",
    "#外部资源\n",
    "#链接：<a href=\"http://www.nju.edu.cn'>\n",
    "#</a>\n",
    "#元素必须闭合与合理包容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<style>等一大块是为了让网页美观等的，所以不用关注\n",
    "#div，标准的块元素\n",
    "#span，标准的内元素\n",
    "#说明性元素，注释：meta，link，script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HTML基本元素\n",
    "#连接，a\n",
    "#图像,img\n",
    "#内容，p\n",
    "#内容，h1-h7\n",
    "#表格，table，tr， td\n",
    "#组织列表，ul ol li\n",
    "#组织内容的方式，dl dt\n",
    "#加粗等，b, strong,i,em,cite\n",
    "#iframe，这是啥?\n",
    "#表单form,input,textarea,select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#连接 a\n",
    "#<a href='地址' 连接显示的内容，</a>\n",
    "#实际地址与显示内容可以独立，不同\n",
    "#连接如果当中有中文，就要转码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#图片img\n",
    "#<img src=\"图片地址\"\n",
    "#常见属性：alt=”图片的文本说明\n",
    "#width，height border ,这些不赞成使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文本：<p>内容<p>   代表一个段落\n",
    "#<h1>内容<h1> 代表一级标题（h2，h3。。。h7）\n",
    "#具体可看以下网页的源代码：http://www.ssfcn.com/detailed_gh.asp?id=36816&sid=2152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#表格\n",
    "#<table>\n",
    "#<tr>\n",
    "#<th>标题1</th><th>标题2</th>\n",
    "#</tr>\n",
    "#<tr>\n",
    "#<td>内容1</td><td>内容2</td>\n",
    "#</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ul 无序列表\n",
    "#ol 有序列表\n",
    "#li 列表细则，li之间为并列关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<dl> 标记定义一个定义列表\n",
    "#<dt> 定义术语的标题\n",
    "#<dd> 定义术语的描述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用户与服务器交互的最主要渠道之一，尤其是文件上传，用户登陆等场合\n",
    "#<form action=\"路径\" method =\"post\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input 注明输入的变量\n",
    "#<input type=\"text\"value =\"\" name=\"变量名”>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#需要说明的是，以上大部分内容都不太需要掌握,并不影响进行爬虫，因为这些信息都不是我们需要的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1.2 网页的获取与阅读"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#网络爬虫流程：\n",
    "#找到HTML网页源代码\n",
    "#分析代码结构，找出数据部分与其他无用部分区分标志\n",
    "#利用区分点设计区分模板（关键步骤）\n",
    "#用Python获得整个网页源代码，运行区分模块代码提炼数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#网页文本的获取\n",
    "#urllib是python的一个获取url（统一资源定位符，Uniform Resource Lacators)的模块，可以用来抓取远程网页\n",
    "#urllib.request.urlopen(url,data,timeout)可以获取页面，\n",
    "#url 为需要打开的网址\n",
    "#data，传递的参数，字典形式，默认为None时为get方法。\n",
    "#urllib.request.urlopen(url,data,timeout)可以获取页面，timeout是响应时间。\n",
    "#获取页面内容的数据格式为bytes类型，需要进行decode解码，转换为str类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#网页文本获取\n",
    "from urllib import request,parse\n",
    "url='http://www.ahmhxc.com/gongzuobaogao/12949.html' \n",
    "resp=request.urlopen(url)\n",
    "html=resp.read()\n",
    "#print(html)   #如果编码格式不对，这个显示都是乱码\n",
    "string=html.decode('gb2312')  #具体编码方式可以看网页最上头\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#另一个例子\n",
    "from urllib import request,parse\n",
    "from bs4 import BeautifulSoup                           #beautifulsoup4 可以让网页源代码更美观，但似乎不太需要\n",
    "url='http://www.ssfcn.com/detailed_gh.asp?id=36816&sid=2152'   #网站是上海现代服务业联合会\n",
    "resp=request.urlopen(url)\n",
    "print(resp)\n",
    "html=resp.read()\n",
    "string=html.decode('utf-8')\n",
    "print(string)  \n",
    "#这个程序如果运行太多，也会被反爬，而且有的网络不能访问这个网站，那就无法打开，比如我家里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#有些标题看起来很凌乱，实际上是中文的转码\n",
    "#http://vip.stock.finance.sina.com.cn/corp/view/vCI_CorpManagerInfo.php?stockid=000651&Name=%B6%AD%C3%F7%D6%E9\n",
    "\n",
    "from urllib import request, parse  \n",
    "url='http://vip.stock.finance.sina.com.cn/corp/view/vCI_CorpManagerInfo.php?stockid=000651&Name='\n",
    "url1=parse.quote('董明珠'.encode('gbk'))  #url包含中文要转码\n",
    "url2=url+url1\n",
    "print(url2)\n",
    "resp=request.urlopen(url2)\n",
    "html=resp.read()\n",
    "string=html.decode('gbk')\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#另一种中读取网页的方式\n",
    "import requests\n",
    "\n",
    "#案例1：# 这个程序如果重复输入，就会被反爬，需要输入验证码了。。\n",
    "#res = requests.get('http://bj.xiaozhu.com/')  #网站是小猪短租网北京地区网址\n",
    "#print(res)  #返回值response[200]，说明请求网址成功，若为404，400，则请求失败【好熟悉的404！】\n",
    "#print(res.text)  \n",
    "\n",
    "#案例2\n",
    "url='http://www.ahmhxc.com/gongzuobaogao/12949.html'\n",
    "res = requests.get(url)  \n",
    "print(res) \n",
    "string=res.content\n",
    "string2=str(string,'gb2312')\n",
    "print(string2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requests常见异常\n",
    "#requests模块不仅有get()方法，还有post()方法。\n",
    "#post()方法用于提交表单来爬取需要登录才能获得数据的网站，见下一讲\n",
    "#requests常见错误：\n",
    "#1 requests抛出一个ConnectionError异常，原因为网络问题（DNS查询失败、拒绝连接等）\n",
    "#2 Response.raise_for_status 抛出一个HTTPError异常，原因为HTTP请求返回了不成功的状态码（如网页不存在，返回404错误）\n",
    "#3 Resquests抛出一个Timeout异常，原因是请求超时\n",
    "#4 Requests抛出一个 TooManyRedirects异常，原因为请求超过了设定的最大重定向次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可以通过try的方式避免异常\n",
    "\n",
    "#通过try和except，如果请求成果，执行程序，如果请求出现ConnectionError异常，则会打印“拒绝连接”（或者其他操作），\n",
    "#通过try和except设置，就可以使得程序不报错，跳过错误，继续执行(pass,continue等），或者报告错误的原因，类型等\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup                          \n",
    "#url='http://www.ssfcn.com/detailed_gh.asp?id=36816&sid=2152'   #网站是上海现代服务业联合会，有些地方可能会拒绝连接\n",
    "url='http://www.ahmhxc.com/gongzuobaogao/12949.html'\n",
    "try:\n",
    "    #resp=requests.get(url)   #这一句如果放在try前面（耿书），有时候会有问题，因为连接不上直接报错，而不是执行try和except程序块\n",
    "    #print(resp.text)\n",
    "    res = requests.get(url)  \n",
    "    string=res.content\n",
    "    print(string)\n",
    "except:\n",
    "    print('拒绝连接')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Header的设置\n",
    "#爬虫最大的障碍就是“反爬虫”\n",
    "#为了突破“反爬虫”，一个小技巧是将程序伪装成浏览器来向网页服务器发起Request（很初级的技术）\n",
    "#浏览器与服务器连接中提交一些浏览器端（header）的信息。\n",
    "#一些服务器通过读取header数据判断是否为爬虫，从而防止服务器被大量无意义读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#header主要属性\n",
    "#Host 服务器站点名\n",
    "#User-agent 浏览器名和版本号等\n",
    "#Referer 下一页地址\n",
    "#Cookie 包含用户名登陆，以及一些安全认证等信息\n",
    "#一般使用User-agent即可\n",
    "# headerf 方法：\n",
    "#1）打开开发者工具【审查元素？】；2，选中Network；3，刷新原始网页；4，“name”中选中一个（中间以后的）；5，最下面就能看到User-Agent\n",
    "\n",
    "#设置开发者工具的方式：\n",
    "#https://jingyan.baidu.com/article/c1465413ff61820bfcfc4cf4.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Header 提交实例：模拟Chrome 浏览器\n",
    "head={}  #字典结构\n",
    "head['User-Agent']='Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Mobile Safari/537.36'\n",
    "req=request.Request(url,headers=head)\n",
    "resp=request.urlopen(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用header的示例\n",
    "from urllib import request,parse\n",
    "\n",
    "path='D:/python/郭峰Python讲义/数据与结果/'\n",
    "\n",
    "\n",
    "url='http://www.ahmhxc.com/gongzuobaogao/12949.html'\n",
    "head={}  #字典结构\n",
    "head['User-Agent']='Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'\n",
    "req=request.Request(url,headers=head)\n",
    "resp=request.urlopen(req)\n",
    "print(resp)\n",
    "html=resp.read()\n",
    "#print(hfrom bs4 import BeautifulSoup tml)\n",
    "string=html.decode('gb2312')  #改成gbk,会报错\n",
    "print(string)\n",
    "f=open(path+'scraping.txt','w')  #可以输出到一个txt文档\n",
    "f.write(string)             \n",
    "\n",
    "#如果加一个循环，就可以将这些报告全部爬下来\n",
    "#所以后面的关键是如何从这些杂乱的信息中筛选有用信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1.3 简单爬虫实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#周耿教科书案例：北京地区短租房信息\n",
    "#本案例和下一个案例都是采用了BeautifulSoup的select的方法来识别网页源代码，比较少用，不学也罢，而且IP已被网站封\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time           #导入相应库文件\n",
    "\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'\n",
    "}                                        #加入请求头\n",
    "\n",
    "def judgment_sex(class_name):           #判定用户性别的函数\n",
    "  if class_name == ['member_ico1']:\n",
    "      return '女'\n",
    "  else:\n",
    "      return  '男'\n",
    "\n",
    "def get_links(url):                    #定义获取详细页url的函数\n",
    "    wb_data = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(wb_data.text,'lxml')\n",
    "    links = soup.select('#page_list > ul > li > a')\n",
    "#    print(links)\n",
    "    for link in links:\n",
    "        href = link.get(\"href\")\n",
    "        print(href)\n",
    "        get_info(href)                 #这里出现真正的具体的url，然后再依次调用get_info()函数\n",
    "\n",
    "def get_info(url):                    #定义获取网页信息的函数\n",
    "    wb_data = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(wb_data.text,'lxml')\n",
    "    tittles = soup.select('div.pho_info > h4')\n",
    "    addresses = soup.select('span.pr5')\n",
    "    prices = soup.select('#pricePart > div.day_l > span')\n",
    "    imgs = soup.select('#floatRightBox > div.js_box.clearfix > div.member_pic > a > img')\n",
    "    names = soup.select('#floatRightBox > div.js_box.clearfix > div.w_240 > h6 > a')\n",
    "    sexs = soup.select('#floatRightBox > div.js_box.clearfix > div.member_pic > div')\n",
    "    for tittle, address, price, img, name, sex in zip(tittles,addresses,prices,imgs,names,sexs):    #zip的方式，下文介绍\n",
    "        data = {\n",
    "            'tittle':tittle.get_text().strip(),\n",
    "            'address':address.get_text().strip(),\n",
    "            'price':price.get_text(),\n",
    "            'img':img.get(\"src\"),\n",
    "            'name':name.get_text(),\n",
    "            'sex':judgment_sex(sex.get(\"class\"))\n",
    "        }\n",
    "        print(data)                       #获取信息并通过字典的信息打印\n",
    "\n",
    "if __name__ == '__main__':               #程序入口，调用函数的方式都是这么写的，但原因不了解\n",
    "    urls = ['http://bj.xiaozhu.com/search-duanzufang-p{}-0/'.format(number) for number in range(1,2)]   #构造多页url(太多会被封)\n",
    "#            http://bj.xiaozhu.com/search-duanzufang-p5-0/ \n",
    "    for single_url in urls:             #循环调用get_links()函数\n",
    "        get_links(single_url)\n",
    "        time.sleep(5)                   #休息2秒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#周耿教科书案例：酷狗音乐排行榜爬取\n",
    "#http://www.kugou.com/yy/rank/home/1-8888.html\n",
    "import requests\n",
    "from bs4 import BeautifulSoup   #去掉from bs4就不行，不同的模块引入方式不同\n",
    "import time                        \n",
    "#headers = {\n",
    "#   'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like 07 Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "#        }              #加入请求头\n",
    "def get_info(url):     #定义获取信息的函数\n",
    "    wb_data = requests.get(url)  #获取网页源代码 表头模式：wb_data = requests.get(url,headers=headers) \n",
    "    soup = BeautifulSoup(wb_data.text,'lxml')    #整理网页源代码\n",
    "    ranks = soup.select('span.pc_temp_num')      #这些路径是怎么获得的，还搞不清楚\n",
    "    titles = soup.select('div.pc_temp_songlist > ul > li > a')\n",
    "    times = soup.select('span.pc_temp_tips_r > span')\n",
    "    for rank,title,time in zip(ranks,titles,times):   #zip是压缩的意思，去掉还不行\n",
    "        data = {\n",
    "            'rank':rank.get_text().strip(),  # 去空格\n",
    "            'singer':title.get_text().split('-')[0],\n",
    "            'song':title.get_text().split('-')[0],  #通过split获取歌手和歌曲信息\n",
    "            'time':time.get_text().strip()\n",
    "        }\n",
    "        print(data)  #获取爬虫信息并按字典格式打印\n",
    "\n",
    "if __name__ == '__main__':  #程序主入口\n",
    "    urls = ['http://www.kugou.com/yy/rank/home/{}-8888.html'.format(str(i)) for i in range(1,24)]   #构造多页url\n",
    "    for url in urls:\n",
    "        get_info(url)  #循环调用get_info函数\n",
    "        time.sleep(1)  #睡眠1秒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip() 函数用于将可迭代对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的对象\n",
    "#学习资料详见https://www.cnblogs.com/wushuaishuai/p/7766470.html\n",
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "c = [4,5,6,7,8]\n",
    "zipped = zip(a,b)     # 打包为元组的列表\n",
    "#list(zipped)         #不加list似乎有问题\n",
    "zip(a,c)              # <zip at 0x29f8df26688>，没问题\n",
    "list(zip(a,c))        #取较短的一个\n",
    "#zip(*zipped)          # 与 zip 相反，可理解为解压，返回二维矩阵式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1.4 正则表达式与源代码解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正则表达式是Python提供的一个强大的可以用来匹配、提取信息的模板\n",
    "#re模块 提供对正则表达式的支持。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先来看一个例子\n",
    "import re\n",
    "#目的:在下面这个看起来杂乱无章的字符串中提取1954这个有用信息\n",
    "string='''<td><div align=\"center\">1954</div></td>'''  \n",
    "pattern='\\\"center\\\">([0-9]+)<\\/div>' #[0-9]+也可以改为\\d+\n",
    "x=re.findall(pattern, string)  #string当中找到所有满足pattern规则部分\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正则表达式举例\n",
    "#[0-9]+ 匹配数字\n",
    "#[\\s]+  匹配空格\n",
    "#[^\\s]+,或者[^<]+ 匹配非空格的字符串\n",
    "#[^\\']+ 匹配非双引号的字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预定义字符集\n",
    "#\\d  匹配一个数字字符，等价于[0-9]\n",
    "#\\D  匹配一个非数字字符，等级与[^0-9]\n",
    "#\\s  匹配任何空白字符，包括空格，制表符、换页符等。\n",
    "#\\S  匹配任何非空白字符\n",
    "#\\w  匹配包括下划线在内的任何单词字符\n",
    "#\\W 匹配任何非单词字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数量词\n",
    "# * 匹配前一个字符0或者无限次\n",
    "# + 匹配前一个字符1或者无限次\n",
    "#？ 匹配前一个字符串0或者1次\n",
    "#{m} 匹配前一个字符m次\n",
    "#{m,n} 匹配前一个字符m至n次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多项数据单次匹配\n",
    "import re\n",
    "info='<a href=\"http://www.baidu.com\">baidu</a>'\n",
    "relink='<a href=\"(.*)\">(.*)</a>'   #也可以写成[^\\\"]+\n",
    "cinfo=re.findall(relink,info)\n",
    "print(cinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#单项数据多次匹配\n",
    "import re\n",
    "string='''<td><div align=\"center\">董明珠</div></td>\n",
    "             <td><div align=\"center\">女</div></td>\n",
    "             <td><div align=\"center\">1954</div></td>\n",
    "             <td><div align=\"center\">研究生 </div></td>\n",
    "         '''\n",
    "#pattern='align=\\\"center\\\">([^<]+)</div></td>'  # 匹配非<字符，一个字符及以上\n",
    "pattern='align=\\\"center\\\">(.*?)</div></td>'  # 这样也可以\n",
    "#pattern='align=\\\"center\\\">([\\S]+)</div></td>'  # 匹配非空字符串，研究生这里有一个空格，所以匹配不出来\n",
    "x=re.findall(pattern, string)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request, parse\n",
    "url='http://vip.stock.finance.sina.com.cn/corp/view/vCI_CorpManagerInfo.php?stockid=000651&Name='\n",
    "url1=parse.quote('董明珠'.encode('gbk'))#url包含中文要转码\n",
    "url2=url+url1\n",
    "print(url2)\n",
    "resp=request.urlopen(url2)\n",
    "html=resp.read()\n",
    "string=html.decode('gbk')\n",
    "#print(string)\n",
    "#pattern='align=\\\"center\\\">([^<]+)<\\/div><\\/td>'\n",
    "#x=re.findall(pattern, string)\n",
    "#print(x)\n",
    "\n",
    "#如果只想匹配最开始的姓名性别等信息\n",
    "pattern=\"align=\\\"center\\\">([^<]+)<\\/div><\\/td>[\\s]+<td><div align=\\\"center\\\">([^<]+)<\\/div><\\/td>[\\s]+<td><div align=\\\"center\\\">([0-9]+)<\\/div><\\/td>[\\s]+\"\n",
    "xn=re.findall(pattern, string)\n",
    "print(xn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#举例\n",
    "string=\"郭峰，1984年3月出生，经济学博士，金融学博士后，现为上海财经大学公共经济与管理学院投资系副教授。。。。\"\n",
    "year=re.findall('(\\d+)年', string)\n",
    "print(year)\n",
    "#有时候可以通过多次批评求得最终结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#除了findall外，re模块还有search(),sub(),find()等函数。\n",
    "#re.search()匹配并提取第一个符合规律的内容。\n",
    "#re.search(pattern,string,flags=0)  \n",
    "#pattern为匹配的正则表达式\n",
    "#string为要匹配的字符串\n",
    "#flags为标志位，控制匹配方式，如是否区分大小写，多行匹配等\n",
    "import re\n",
    "a='one1two2three3'\n",
    "infos=re.search('\\d+',a)\n",
    "print(infos.group())\n",
    "#group（）在正则表达式中用于获取分段截获的字符串，解释如下代码\n",
    "#其中，group（0）和group（）效果相同，均为获取取得的字符串整体。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group示例\n",
    "import re\n",
    "a = \"123abc456\"\n",
    "print(re.search(\"([0-9]*)([a-z]*)([0-9]*)\",a).group(0))    #123abc456,返回整体\n",
    "print(re.search(\"([0-9]*)([a-z]*)([0-9]*)\",a).group(1))   #123\n",
    "print(re.search(\"([0-9]*)([a-z]*)([0-9]*)\",a).group(2))    #abc\n",
    "print(re.search(\"([0-9]*)([a-z]*)([0-9]*)\",a).group(3))    #456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re.sub()函数用于替换字符串中的匹配项，sub()函数的语法为：\n",
    "#re.sub(pattern,repl,string,count=0,flags=0)\n",
    "#pattern为匹配的正则表达式\n",
    "#repl为替换的字符串，string为要被查找替换的原始字符串\n",
    "#为匹配后替换的最大次数，默认0为替换所有匹配\n",
    "#flags为标志位，控制匹配方式，如是否区分大小写，多行匹配等\n",
    "import re\n",
    "phone='123-4567-1234'\n",
    "new_phone=re.sub('\\D',\"\",phone)\n",
    "print(new_phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re模块修饰符\n",
    "#re.I 使匹配对大小写不敏感\n",
    "#re.L 作本地化识别\n",
    "#re.M 多行匹配\n",
    "#re.S 使匹配包括换行在内的多有字符，这个最有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#知县网\n",
    "url='http://www.ahmhxc.com/gongzuobaogao/12949.html'\n",
    "res = requests.get(url)  \n",
    "print(res) \n",
    "string=res.content\n",
    "string2=str(string,'gb2312')\n",
    "string3=re.findall(' <script src=\"/d/js/acmsd/thea8.js\"></script>(.*?)<div class=\"pagebreak\">',string2,re.S)\n",
    "print(string3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1.5 更多爬虫实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#罗攀著作提供的案例，这个例子比较简单\n",
    "#斗破苍穹小说网中该小说的全文信息（http://www.doupoxs.com/doupocangqiong/）\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "\n",
    "path='D:/python/郭峰Python讲义/数据与结果/'\n",
    "\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "}\n",
    "\n",
    "\n",
    "f = open(path+'doupo.txt','a+')  \n",
    "\n",
    "def get_info(url):\n",
    "    res = requests.get(url,headers=headers)\n",
    "    if res.status_code == 200:            #网页编码有跳跃，部分404\n",
    "        print(url)                        #追加一些输出，可以监控爬虫进程\n",
    "        contents = re.findall('<p>(.*?)</p>',res.content.decode('utf-8'),re.S)  #跨行匹配\n",
    "        for content in contents:\n",
    "            f.write(content+'\\n')        #正则获取数据，写入txt文件\n",
    "    else:\n",
    "        pass                            #不为200，就pass掉\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    urls = ['http://www.doupoxs.com/doupocangqiong/{}.html'.format(str(i)) for i in range(2,1665)]   #网页编码规则有跳跃\n",
    "    for url in urls:\n",
    "        get_info(url)\n",
    "        time.sleep(1)              #爬行间隔时间，1600多页，爬完需要一点时间。\n",
    "    f.close()      \n",
    "#注意：运行完之后，txt文件中存在一些多余特殊字符，可以通过替换等方式将其剔除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#罗攀著作案例：糗事百科的段子信息\n",
    "#http://www.qiushibaike.com/text/page/2/\n",
    "#https://www.qiushibaike.com/text/page/1/,即为https://www.qiushibaike.com/text/\n",
    "import requests\n",
    "import re\n",
    "path='D:/python/郭峰Python讲义/数据与结果/'\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'\n",
    "}\n",
    "\n",
    "info_lists = []             #初始化列表，用于装入爬虫信息\n",
    "\n",
    "def judgment_sex(class_name):\n",
    "  if class_name == 'womenIcon':\n",
    "      return '女'\n",
    "  else:\n",
    "      return  '男'\n",
    "\n",
    "def get_info(url):\n",
    "    res = requests.get(url)\n",
    "    ids = re.findall('<h2>(.*?)</h2>',res.text,re.S)\n",
    "    levels = re.findall('<div class=\"articleGender \\D+Icon\">(.*?)</div>',res.text,re.S)\n",
    "    sexs = re.findall('<div class=\"articleGender (.*?)\">',res.text,re.S)\n",
    "    contents = re.findall('<div class=\"content\">.*?<span>(.*?)</span>',res.text,re.S)\n",
    "    laughs = re.findall('<span class=\"stats-vote\"><i class=\"number\">(\\d+)</i>',res.text,re.S)\n",
    "    comments = re.findall('<i class=\"number\">(\\d+)</i> 评论',res.text,re.S)\n",
    "    for id,level,sex,content,laugh,comment in zip(ids,levels,sexs,contents,laughs,comments):\n",
    "        info = {\n",
    "            'id':id,\n",
    "            'level':level,\n",
    "            'sex':judgment_sex(sex),\n",
    "            'content':content,\n",
    "            'laugh':laugh,\n",
    "            'comment':comment\n",
    "        }\n",
    "        info_lists.append(info)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    urls = ['http://www.qiushibaike.com/text/page/{}/'.format(str(i)) for i in range(1,13)]\n",
    "    for url in urls:\n",
    "        get_info(url)\n",
    "    for info_list in info_lists:\n",
    "        print(info_list)\n",
    "        f = open(path+'qiushi.txt','a+')\n",
    "        try:                                    #文字编码可能存在错误，try except会导致只存入没问题的文字，有文字的被pass掉了\n",
    "             f.write(info_list['id']+'\\n')\n",
    "             f.write(info_list['level'] + '\\n')\n",
    "             f.write(info_list['sex'] + '\\n')\n",
    "             f.write(info_list['content'] + '\\n')\n",
    "             f.write(info_list['laugh'] + '\\n')\n",
    "             f.write(info_list['comment'] + '\\n\\n')\n",
    "             f.close()\n",
    "        except UnicodeEncodeError:\n",
    "             pass        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#周耿讲义提供的例子：学区房房价\n",
    "from urllib import request, parse\n",
    "import re\n",
    "import pandas as pd   #这个案例使用了pandas\n",
    "data_xx=[]\n",
    "data_zx=[]\n",
    "for i in range(1,2): #修改为实际的页码\n",
    "    url=\"http://nj.sell.house365.com/school/sl_p\"+str(i)+\"-n22.html\"\n",
    "    print(url)\n",
    "    head = {}\n",
    "    head['User-Agent'] = 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19'\n",
    "    req = request.Request(url, headers=head)\n",
    "    response = request.urlopen(req)   \n",
    "    html=response.read()\n",
    "    string=html.decode('gbk')\n",
    "    arr=string.split('class=\\\"info_list clearfix')\n",
    "for string1 in arr[1:]:\n",
    "        #print(string1)\n",
    "        name=re.findall(\"<a class=\\\"schooltitle\\\" href=\\\"[^\\\"]+\\\" target=\\\"_blank\\\">([^<]+)\",string1)\n",
    "        price=re.findall(\">([0-9.]+)元\\/㎡<\\/span>\",string1)\n",
    "        p=0\n",
    "        for x in price:\n",
    "            p+=float(x)\n",
    "        if(len(price)>0):\n",
    "            p=p/len(price)  \n",
    "        if name[0].find('小学')>0:\n",
    "            data_xx.append((name[0],p))\n",
    "        else:\n",
    "            data_zx.append((name[0],p))\n",
    "df1=pd.DataFrame(data_xx,columns=['小学','房价'])\n",
    "df2=pd.DataFrame(data_zx,columns=['中学','房价'])\n",
    "print(df1), print(df2)\n",
    "#df1.to_csv('e:/njxx.csv',encoding='gbk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
