{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python语言与经济大数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5_2讲 网络爬虫进阶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 郭峰（上海财经大学投资系副教授）\n",
    "#Email：guo.feng@mail.sufe.edu.cn\n",
    "#2019-2-27\n",
    "#本讲目录\n",
    "#5.2.1 异步加载式网页的爬取:2345天气网\n",
    "#5.2.2 表单交互（post方法）：环保部AQI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2.1 异步加载式网页的爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对于异步加载的网页，数据不在“网页源代码”\n",
    "#例如简书网：https://www.jianshu.com/c/bDHhpK\n",
    "#网页加载这些数据的过程也称为“逆行工程”\n",
    "#以2345天气网来讲解这类网站的爬取方法，http://tianqi.2345.com/\n",
    "#第一步，打开浏览器的开发者工具（部分电脑为F12键），选择Network选项\n",
    "#第二步，刷新或者点击下一页，可以出现相关信息\n",
    "#第三步，在headers部分可以看到请求的url，从中可以找到相关规律\n",
    "#第四步，preview当中可以找到相关数据\n",
    "#目前只搞定一个2345天气网的爬虫，罗攀著作提供的案例就没搞定。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 【实际应用】2345天气网爬取\n",
    "#先爬一个id列表\n",
    "from urllib import request, parse\n",
    "from urllib.request import urlopen\n",
    "import time  \n",
    "import re\n",
    "import requests\n",
    "import csv\n",
    "head={}  #字典结构\n",
    "head['User-Agent']='Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'\n",
    "#读取id\n",
    "path='D:/python/tianqi//' \n",
    "f=open(path+'cityid'+'.csv','a',encoding='gb2312') \n",
    "c=csv.writer(f)\n",
    "#c.writerow(('city_id','city_1','city_2'))            \n",
    "for id in range(72129,73000):       #最高到72129\n",
    "    url='http://tianqi.2345.com/wea_history/'+str(id)+'.htm'\n",
    "#        http://tianqi.2345.com/wea_history/71445.htm       #模仿此公式计算而来\n",
    "    print(url)\n",
    "    req=requests.get(url,headers=head)\n",
    "    if req.status_code==200:\n",
    "        resp=request.urlopen(url)\n",
    "        html=resp.read()\n",
    "        string=html.decode(encoding='gb2312',errors='ignore')  \n",
    "#        print(string)\n",
    "        city1=re.findall('<a href=\"/\" title=\"天气预报\">天气预报</a><span>&gt;</span><a title=\"(.*?)天气预报\" href',string) \n",
    "#       <a href=\"/\" title=\"天气预报\">天气预报</a><span>&gt;</span><a title=\"青海天气预报\" href=\"/qinghai_dz/30.htm\" \n",
    "        city2=re.findall('id=\"lastBread\">(.*?)历史天气</a>',string,re.S)    \n",
    "#                         id=\"lastBread\">德令哈历史天气</a>\n",
    "        city=city1+city2           #这是个列表，每个元素是一串文字，而不是一个\n",
    "        print(id,city1,city2)\n",
    "        c.writerow((id,city1[0],city2[0]))\n",
    "    else:\n",
    "        continue\n",
    "    time.sleep(2)           #爬取间隔数据\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【实际应用】2345天气网爬取\n",
    "#正式爬取\n",
    "from urllib import request, parse\n",
    "from urllib.request import urlopen\n",
    "import time  \n",
    "import re\n",
    "import requests\n",
    "import csv\n",
    "import http.cookiejar\n",
    "path='D:/python/tianqi/' \n",
    "pathdata='D:/python/tianqi/data/'\n",
    "\n",
    "head={}  #字典结构\n",
    "head['User-Agent']='Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'\n",
    "\n",
    "cityid=open(path+'cityidnew.csv','r')\n",
    "cityid=cityid.read()\n",
    "cityids=cityid.split('\\n')\n",
    "cityids=cityids[0:370]        #多出一行，末尾为空，搞不清楚原因，可能是上一行\\n切割导致的 \n",
    "    \n",
    "cityname=open(path+'cityname.csv','r')\n",
    "cityname=cityname.read()\n",
    "citynames=cityname.split('\\n')\n",
    "\n",
    "month=open(path+'monthn.csv','r')\n",
    "month=month.read()\n",
    "mons=month.split('\\n')\n",
    "mons=mons[0:64]\n",
    "print(len(cityids),len(mons))\n",
    "#print(mons)\n",
    "i=369\n",
    "while i<len(cityids):   #不使用for语言，是为了更好地监控爬虫进程，以及出错后的后续跟进\n",
    "    print(i,citynames[i])\n",
    "    f=open(pathdata+'_'+str(i)+'_'+citynames[i]+'.csv','w+',encoding='gb2312') \n",
    "    c=csv.writer(f)\n",
    "    c.writerow(('city_id','city_name','date','wenduh','wendul','tianqi','fengxiang','fengli','aqi'))\n",
    "    for mon in mons:\n",
    "        if mons.index(mon)<38:  #2016年3月前和后网址不太一样。\n",
    "            url = \"http://tianqi.2345.com/t/wea_history/js/\"+cityids[i]+'_'+mon+'.js'\n",
    "        else:\n",
    "            url = \"http://tianqi.2345.com/t/wea_history/js/\"+mon+'/'+cityids[i]+'_'+mon+'.js'\n",
    "#             http://tianqi.2345.com/t/wea_history/js/201803/71799_201803.js\n",
    "        print(url)\n",
    "        req=requests.get(url,headers=head)\n",
    "        resp=request.urlopen(url)\n",
    "        html=resp.read()\n",
    "        string=html.decode(encoding='gb2312',errors='ignore')  \n",
    "#            print(string)\n",
    "        city=re.findall('city:\\'(.*?)\\',tqInfo',string)\n",
    "        if mons.index(mon)<36:         #2015年12月前和后的数据不一样，之后有aqi，之前没有。\n",
    "            data=re.findall('ymd:\\'(.*?)\\',bWendu:\\'(.*?)\\',yWendu:\\'(.*?)\\',tianqi:\\'(.*?)\\',fengxiang:\\'(.*?)\\',fengli:\\'(.*?)\\'}',string,re.S)\n",
    "        else:\n",
    "            data=re.findall('ymd:\\'(.*?)\\',bWendu:\\'(.*?)\\',yWendu:\\'(.*?)\\',tianqi:\\'(.*?)\\',fengxiang:\\'(.*?)\\',fengli:\\'(.*?)\\',aqi:\\'(.*?)\\',aqiInfo',string,re.S)\n",
    "#        {city:'鼓楼',tqInfo:[{ymd:'2018-03-01',bWendu:'14℃',yWendu:'4℃',tianqi:'多云~小雨',fengxiang:'东北风',fengli:'4-5级',aqi:'91',aqiInfo:'良',aqiLevel:'2'}\n",
    "        for d in range(0,len(data)):\n",
    "            if mons.index(mon)<36:\n",
    "                c.writerow((cityids[i],city[0],data[d][0],data[d][1],data[d][2],data[d][3],data[d][4],data[d][5]))\n",
    "            else:\n",
    "                c.writerow((cityids[i],city[0],data[d][0],data[d][1],data[d][2],data[d][3],data[d][4],data[d][5],data[d][6])) \n",
    "        time.sleep(0.5) #爬取间隔数据\n",
    "    f.close()\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【实际应用】2018年5月和6月的2345天气网爬取\n",
    "from urllib import request, parse\n",
    "from urllib.request import urlopen\n",
    "import time  \n",
    "import re\n",
    "import requests\n",
    "import csv\n",
    "import http.cookiejar\n",
    "path='D:/python/tianqi/' \n",
    "pathdata='D:/python/tianqi/data2/'\n",
    "\n",
    "head={}  #字典结构\n",
    "head['User-Agent']='Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'\n",
    "\n",
    "cityid=open(path+'cityidnew.csv','r')    #城市在天气网中的对应代码\n",
    "cityid=cityid.read()\n",
    "cityids=cityid.split('\\n')\n",
    "cityids=cityids[0:370]        #多出一行，末尾为空，搞不清楚原因，可能是上一行\\n切割导致的 \n",
    "    \n",
    "cityname=open(path+'cityname.csv','r')   #城市名称，与cityid一一对应\n",
    "cityname=cityname.read()\n",
    "citynames=cityname.split('\\n')\n",
    "\n",
    "mons=['201805','201806']\n",
    "\n",
    "f=open(pathdata+'mon1805'+'_'+'mon1806'+'.csv','w+',encoding='gb2312') \n",
    "c=csv.writer(f)\n",
    "c.writerow(('city_id','city_name','date','wenduh','wendul','tianqi','fengxiang','fengli','aqi'))\n",
    "\n",
    "i=0\n",
    "\n",
    "while i<len(cityids):   #不使用for语言，是为了更好地监控爬虫进程，以及出错后的后续跟进\n",
    "    print(i,citynames[i])\n",
    "    for mon in mons:\n",
    "        url = \"http://tianqi.2345.com/t/wea_history/js/\"+mon+'/'+cityids[i]+'_'+mon+'.js'\n",
    "#             http://tianqi.2345.com/t/wea_history/js/201803/71799_201803.js\n",
    "        print(url)\n",
    "        req=requests.get(url,headers=head)\n",
    "        resp=request.urlopen(url)\n",
    "        html=resp.read()\n",
    "        string=html.decode(encoding='gb2312',errors='ignore')  \n",
    "#            print(string)\n",
    "        city=re.findall('city:\\'(.*?)\\',tqInfo',string)\n",
    "        data=re.findall('ymd:\\'(.*?)\\',bWendu:\\'(.*?)\\',yWendu:\\'(.*?)\\',tianqi:\\'(.*?)\\',fengxiang:\\'(.*?)\\',fengli:\\'(.*?)\\',aqi:\\'(.*?)\\',aqiInfo',string,re.S)\n",
    "#        {city:'鼓楼',tqInfo:[{ymd:'2018-03-01',bWendu:'14℃',yWendu:'4℃',tianqi:'多云~小雨',fengxiang:'东北风',fengli:'4-5级',aqi:'91',aqiInfo:'良',aqiLevel:'2'}\n",
    "        for d in range(0,len(data)):\n",
    "            c.writerow((cityids[i],city[0],data[d][0],data[d][1],data[d][2],data[d][3],data[d][4],data[d][5],data[d][6])) \n",
    "        time.sleep(0.1) #爬取间隔数据\n",
    "    i=i+1\n",
    "f.close()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【实际应用】2345天气网个别地区的爬取,这些地区aqi数据一会有一会没有,\n",
    "#或者早期样本缺失\n",
    "#包括201805、201806两个年份\n",
    "from urllib import request, parse\n",
    "from urllib.request import urlopen\n",
    "import time  \n",
    "import re\n",
    "import requests\n",
    "import csv\n",
    "import http.cookiejar\n",
    "path='D:/python/tianqi/' \n",
    "pathdata='D:/python/tianqi/data2/'\n",
    "\n",
    "head={}  #字典结构\n",
    "head['User-Agent']='Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'\n",
    "\n",
    "\n",
    "cityid='71444'\n",
    "cityname='三沙'\n",
    "\n",
    "month=open(path+'monthn2.csv','r')\n",
    "month=month.read()\n",
    "mons=month.split('\\n')\n",
    "mons=mons[0:66]\n",
    "#print(mons)\n",
    "\n",
    "f=open(pathdata+'_'+cityname+'.csv','w+',encoding='gb2312',newline='') \n",
    "c=csv.writer(f)\n",
    "c.writerow(('city_id','city_name','date','wenduh','wendul','tianqi','fengxiang','fengli','aqi'))\n",
    "for mon in mons:\n",
    "    if mons.index(mon)<38:  #2016年3月前和后网址不太一样。\n",
    "        url = \"http://tianqi.2345.com/t/wea_history/js/\"+cityid+'_'+mon+'.js'\n",
    "    else:\n",
    "        url = \"http://tianqi.2345.com/t/wea_history/js/\"+mon+'/'+cityid+'_'+mon+'.js'\n",
    "#       http://tianqi.2345.com/t/wea_history/js/201803/71799_201803.js\n",
    "    print(url)\n",
    "    try:\n",
    "        req=requests.get(url,headers=head)\n",
    "        resp=request.urlopen(url)\n",
    "        html=resp.read()\n",
    "        string=html.decode(encoding='gb2312',errors='ignore')  \n",
    "#       print(string)\n",
    "        city=re.findall('city:\\'(.*?)\\',tqInfo',string)\n",
    "        data=re.findall('ymd:\\'(.*?)\\',bWendu:\\'(.*?)\\',yWendu:\\'(.*?)\\',tianqi:\\'(.*?)\\',fengxiang:\\'(.*?)\\',fengli:\\'(.*?)\\'',string,re.S)\n",
    "#        {city:'鼓楼',tqInfo:[{ymd:'2018-03-01',bWendu:'14℃',yWendu:'4℃',tianqi:'多云~小雨',fengxiang:'东北风',fengli:'4-5级',aqi:'91',aqiInfo:'良',aqiLevel:'2'}\n",
    "    #    city:'资阳',tqInfo:[{ymd:'2017-05-01',bWendu:'30℃',yWendu:'21℃',tianqi:'多云~雷阵雨',fengxiang:'无持续风向',fengli:'微风'}\n",
    "        for d in range(0,len(data)):\n",
    "            c.writerow((cityid,city[0],data[d][0],data[d][1],data[d][2],data[d][3],data[d][4],data[d][5]))\n",
    "    except:\n",
    "        continue\n",
    "    time.sleep(0.1) #爬取间隔数据\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#天气数据合并\n",
    "import os\n",
    "import csv\n",
    "path='D:/python/tianqi/' \n",
    "pathdata='D:/python/tianqi/datatotal/'\n",
    "fn=open(path+'tianqi.csv','w+',encoding='gb2312',newline='') \n",
    "cw=csv.writer(fn)\n",
    "cw.writerow(('city_id','city_name', 'date','wenduh','wendul','tianqi','fengxiang','fengli','aqi'))\n",
    "\n",
    "files= os.listdir(pathdata)  #得到文件夹下的所有文件名称  \n",
    "for file in files:    #遍历文件夹 \n",
    "    f=open(pathdata+file,'r',encoding='gb2312') \n",
    "    cr=csv.reader(f)\n",
    "    for row in cr:    \n",
    "        if row==[] or row[0]=='city_id':\n",
    "            continue\n",
    "        else:\n",
    "            if len(row)==8:\n",
    "                rown=[row[0],row[1],row[2],row[3][:-1],row[4][:-1],row[5],row[6],row[7]]\n",
    "            else:\n",
    "                rown=[row[0],row[1],row[2],row[3][:-1],row[4][:-1],row[5],row[6],row[7],row[8]]\n",
    "            cw.writerow(rown)\n",
    "    f.close()\n",
    "fn.close()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#中间步骤\n",
    "import csv\n",
    "pathdata='D:/python/tianqi/data/'\n",
    "f=open(pathdata+'_0_北京.csv','r',encoding='gb2312') \n",
    "cr = csv.reader(f)\n",
    "for row in cr:\n",
    "    if row==[] or row[0]=='city_id':\n",
    "        continue\n",
    "    else:\n",
    "        print(row[0])\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2.2 表单交互：环保部AQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 【实际应用】爬取环保部数据中心AQI数据，post格式。（！！！只能缓慢爬取，容易被封）\n",
    "import urllib\n",
    "import http.cookiejar\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "pathdata='D:/python/aqi/data/'\n",
    "\n",
    "#整理header\n",
    "header = {    \n",
    "\"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"Accept-Language\":\"zh,zh-CN;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"Connection\":\"keep-alive\",\n",
    "    \"Host\":\"datacenter.mep.gov.cn\",\n",
    "    \"Origin\": \"http://datacenter.mep.gov.cn\",        \n",
    "    \"Referer\":\"http://datacenter.mep.gov.cn/websjzx/report!list.vm?xmlname=1512478367400&roleType=CFCD2084&permission=null\",\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\",\n",
    "    \"Upgrade-Insecure-Requests\":1 ,\n",
    "    }\n",
    "dd=open('D:/python/aqi/date.csv','r')\n",
    "dates=dd.read().split('\\n')\n",
    "d=15\n",
    "break_flag=False\n",
    "while d<len(dates):\n",
    "    f=open(pathdata+'_'+dates[d]+'_aqi.csv','w+',encoding='gb2312') \n",
    "    c=csv.writer(f)\n",
    "    c.writerow(('city_name','aqi','level','prime','date'))\n",
    "    print(d,dates[d])\n",
    "    for j in range(37):\n",
    "        url = \"http://114.251.10.135/websjzx/report/list.vm\"\n",
    "        postdata =urllib.parse.urlencode({\n",
    "            \"pageNum\": j,\n",
    "            \"xmlname\":\"1512478367400\",\n",
    "            \"queryflag\":\"close\",\n",
    "            \"customquery\": \"false\",  \n",
    "            \"isdesignpatterns\":\"false\",\n",
    "            \"roleType\": \"CFCD2084\",\n",
    "            \"permission\":\"0\",   \n",
    "            \"V_DATE\":dates[d],\n",
    "            \"inPageNo\":\"1\"\n",
    "        }).encode('utf-8')\n",
    "        req = urllib.request.Request(url,postdata,header)\n",
    "        cj = http.cookiejar.CookieJar()\n",
    "        opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj))\n",
    "        r = opener.open(req)\n",
    "        str1=r.read().decode('utf-8') \n",
    "        cityname=re.findall(\"style='color:blue;'>(.*?)</a></td>\",str1)\n",
    "        aqi=re.findall('<span style=\\'[^\\']+\\'>([0-9]+)',str1)             #这一个始终没看懂，来自于周耿讲义\n",
    "        prime=re.findall('colid=\"5\"  rowspan=\"1\"  colspan=\"1\" style=\"text-align:center;WORD-WRAP: break-word;\">(.*?)</td>',str1)\n",
    "        level=re.findall(\"<span style=\\'[^\\']+\\'>([\\D]+)</span></td>\",str1)\n",
    "        date=re.findall('colid=\"6\"  rowspan=\"1\"  colspan=\"1\" style=\"text-align:center;WORD-WRAP: break-word;\">(.*?)</td>',str1)\n",
    "        for i in range(len(aqi)):\n",
    "            c.writerow((cityname[i],aqi[i],level[i],prime[i],date[i]))\n",
    "        time.sleep(random.choice([15,10,5,20]))\n",
    "        if aqi==[]:\n",
    "            break_flag=True\n",
    "            print('break_flag is',break_flag)\n",
    "            break\n",
    "        else:\n",
    "            print(date[i],j,'keep going')\n",
    "            continue\n",
    "    if break_flag==True:\n",
    "        break\n",
    "    else:\n",
    "        d=d+1\n",
    "    f.close()\n",
    "    time.sleep(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
