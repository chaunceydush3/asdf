{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python语言与经济大数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5_3讲  网络爬虫实战案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 郭峰（上海财经大学投资系副教授）\n",
    "#Email：guo.feng@mail.sufe.edu.cn\n",
    "#2018-07-16\n",
    "#本讲目录\n",
    "#5.3.1 政府工作报告-上海现代服务业联合会\n",
    "#5.3.2 政府工作报告-礼拜五秘书网\n",
    "#5.3.3 政府工作报告-知县网\n",
    "#5.3.4 国家哲学社会科学学术期刊\n",
    "#5.3.5 中国知网\n",
    "#5.3.6 国家哲学社会科学学术期刊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【实际应用案例】：政府工作报告的爬取,来自上海现代服务业联合会\n",
    "from urllib import request, parse\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError,URLError\n",
    "from bs4 import BeautifulSoup \n",
    "import time  \n",
    "import re\n",
    "head={}  #字典结构\n",
    "head['User-Agent']='Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'\n",
    "\n",
    "for i in range(37000,37015): #最高37015\n",
    "    url='http://www.ssfcn.com/detailed_gh.asp?id='+str(i)+'&sid='+'\\d+'\n",
    "#   url='http://www.ssfcn.com/detailed_gh.asp?id=36816&sid=2152'模仿此公式计算而来,搞半天才发现“6&sid=2152”是没有意义的。\n",
    "    print(url)\n",
    "    try:\n",
    "        req=request.Request(url,headers=head)\n",
    "        resp=request.urlopen(url)\n",
    "        html=resp.read()\n",
    "        string=html.decode(encoding='utf-8',errors='ignore')  #格式：str.decode(encoding='UTF-8',errors='strict')       \n",
    "#        print(string)\n",
    "        title=re.findall('<h1 style=\" font-size:14px; color:#990000; padding-top:5px\">(.*?)</h1>',string)\n",
    "        print(title)\n",
    "        date=re.findall('<P align=center>(.*?)</P>',string,re.S)\n",
    "#        <P align=center>太原市市长&nbsp; 李荣怀</P>\n",
    "#        print(date)\n",
    "        text=re.findall('<P>(.*?)</P>',string,re.S)\n",
    "#        print(text)\n",
    "        contents=title+date+text                   #这是个列表，每个元素是一串文字，而不是一个\n",
    "#        for content in contents:\n",
    "#            print(content+'\\n')\n",
    "        path='D:/python/baogao_county/'\n",
    "        if title==[]:                 #个别文档标题为[]，所以先判断一下。\n",
    "            break\n",
    "        bgtitle=\"\"\n",
    "        if  '政府工作报告' in title[0]:    #政府工作报告输出全标题，其他不输出，否则老出错！！！\n",
    "            bgtitle=title[0]\n",
    "        f=open(path+'_'+str(i)+'_'+bgtitle+'.txt','w+',encoding='utf-8')  #不加这个encoding，会出错，真是累死人\n",
    "        for content in contents:\n",
    "            f.write(content+'\\n')\n",
    "        f.close()\n",
    "        time.sleep(1)           #爬取间隔数据，逐渐调试,最后发现2也还是会被封。\n",
    "    except HTTPError as e:\n",
    "        pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t='_18994_2006年自贡市富顺县政府工作报告.txt'\n",
    "a=t.find('年')\n",
    "print(t[1:a-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【实例操作】上海现代服务业联合会删除无用东西，然后放入另一个文件夹\n",
    "import os    \n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import shutil\n",
    "path='D:/python/baogao_county/lianhehui/'\n",
    "pathn='D:/python/baogao_county/lianhehuinew/'\n",
    "files= os.listdir(path)  #得到文件夹下的所有文件名称\n",
    "#print(files)  #这是一个列表\n",
    "for file in files:\n",
    "    if '政府工作报告' in file:\n",
    "        f=open(path+str(file),encoding='utf-8')\n",
    "        try:\n",
    "            text=f.read()   #个别文件存在乱码 ，所以跳过去\n",
    "        except:\n",
    "            continue\n",
    "        text=text.replace('&nbsp;',' ')\n",
    "        text=text.replace('<BR>','\\n  ')\n",
    "        text=text.replace('<br>','')\n",
    "        text=text.replace('</p>','')\n",
    "        text=text.replace('<p>','')\n",
    "        text=text.replace('</div>','')\n",
    "        text=text.replace('</STRONG>','')\n",
    "        text=text.replace('<STRONG>','')\n",
    "        text=text.replace('<p align=\"center\">','')\n",
    "        fn=open(pathn+str(file),'w+',encoding='utf-8')\n",
    "        fn.write(text)\n",
    "        f.close()\n",
    "        os.remove(path+str(file))  #修改一个，删除一个，这样留下来的文件就好弄了\n",
    "        fn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#部分网页源代码和别人不一样，只识别出标题，需要改造重爬\n",
    "from urllib import request, parse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup \n",
    "import time  \n",
    "import re\n",
    "import os    \n",
    "import sys\n",
    "import pickle\n",
    "import  codecs\n",
    "path='D:/python/baogao_county/lianhehuinew/'\n",
    "files= os.listdir(path)  #得到文件夹下的所有文件名称\n",
    "\n",
    "head={}  #字典结构\n",
    "head['User-Agent']='Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'\n",
    "\n",
    "for file in files:\n",
    "    f=open(path+file,'r',encoding='utf-8')\n",
    "    try:\n",
    "        s=f.read()\n",
    "    except:\n",
    "        continue\n",
    "    if len(s)<500:            #小于500字的全都重爬一遍。\n",
    "        filen=file.replace(' ','')\n",
    "        a=filen.find('年')\n",
    "        i=filen[1:a-5]\n",
    "        url='http://www.ssfcn.com/detailed_gh.asp?id='+str(i)+'&sid='+'\\d+'\n",
    "        print(url)\n",
    "        req=request.Request(url,headers=head)\n",
    "        resp=request.urlopen(url)\n",
    "        html=resp.read()\n",
    "        string=html.decode(encoding='utf-8',errors='ignore')  \n",
    "        title=re.findall('<h1 style=\" font-size:14px; color:#990000; padding-top:5px\">(.*?)</h1>',string,re.S)\n",
    "        print(title)\n",
    "        text=re.findall('<div style=\"padding-top:16px\">(.*?) <div style=\"width:980px',string,re.S)\n",
    "       #        print(text)\n",
    "        contents=title+text                   #这是个列表，每个元素是一串文字，而不是一个\n",
    "#        for content in contents:\n",
    "#            print(content+'\\n')\n",
    "        path2='D:/python/baogao_county/lianhehui2/'\n",
    "        fn=open(path2+'_'+str(i)+'_'+title[0]+'.txt','w+',encoding='utf-8')  #不加这个encoding，会出错，真是累死人\n",
    "        for content in contents:\n",
    "            fn.write(content+'\\n')\n",
    "        fn.close()\n",
    "        time.sleep(1.5)\n",
    "        f.close()\n",
    "        olddir=os.path.join(path,file)\n",
    "        os.remove(olddir) \n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【实际应用案例】：政府工作报告的爬取,礼拜五秘书网\n",
    "#先计算一个工作报告的id\n",
    "from urllib import request, parse\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError,URLError\n",
    "from bs4 import BeautifulSoup \n",
    "import time  \n",
    "import re\n",
    "import requests\n",
    "head={}  #字典结构\n",
    "head['User-Agent']='Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'\n",
    "f=open(path+'id.txt','w+',encoding='utf-8') \n",
    "for i in range(1,208):   #到2018年4月22日，一共207页。\n",
    "    url='http://www.libaiwu.com/tag/zfgzbgs/page/'+str(i)+'/'\n",
    "# https://www.libaiwu.com/tag/zfgzbgs/page/2/\n",
    "#    print(url)\n",
    "    req=requests.get(url,headers=head)\n",
    "    if req.status_code==200:\n",
    "        resp=request.urlopen(url)\n",
    "        html=resp.read()\n",
    "        string=html.decode(encoding='utf-8',errors='ignore')  \n",
    "#    print(string)\n",
    "        ids=re.findall(' href=\"https://www.libaiwu.com/(\\d+).htm',string) \n",
    "#        print(ids)\n",
    "#  href=\"https://www.libaiwu.com/3235.htm\">\n",
    "        path='D:/python/baogao_county/libaiwu/'    \n",
    "        for id in ids:\n",
    "            f.write(id+'\\n')\n",
    "    else:\n",
    "        continue\n",
    "    time.sleep(0.5)    #爬取间隔数据\n",
    "f.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【实际应用案例】：政府工作报告的爬取,礼拜五秘书网\n",
    "#正式爬取\n",
    "from urllib import request, parse\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError,URLError\n",
    "from bs4 import BeautifulSoup \n",
    "import time  \n",
    "import re\n",
    "import requests\n",
    "head={}  #字典结构\n",
    "head['User-Agent']='Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'\n",
    "#读取id\n",
    "path='D:/python/baogao_county/libaiwu/' \n",
    "ids=open('D:/python/baogao_county/libaiwu/id.txt','r')\n",
    "iss=ids.read()\n",
    "iss=iss.split('\\n')\n",
    "print(iss[1:10])\n",
    "for id in iss: \n",
    "    url='http://www.libaiwu.com/'+id+'.htm'\n",
    "#        https://www.libaiwu.com/2575.htm       #模仿此公式计算而来，谨慎：部分报告有多页！！！数量较少，没有考虑\n",
    "    print(url)\n",
    "    req=requests.get(url,headers=head)\n",
    "    if req.status_code==200:\n",
    "        resp=request.urlopen(url)\n",
    "        html=resp.read()\n",
    "        string=html.decode(encoding='utf-8',errors='ignore')  \n",
    "#    print(string)\n",
    "        title=re.findall('<title>(.*?) - 礼拜五秘书网</title>',string)  \n",
    "#  <title>沈阳市2013年政府工作报告 - 礼拜五秘书网</title>\n",
    "        print(title)\n",
    "        if '政府工作报告' in title[0]:\n",
    "            stringn=re.sub('<table class=\"wenxue\" width=\"100%\">(.*?)</html>','',string,flags=re.S)\n",
    "#        print(stringn)                                 #这里不删减一些的话，下一句匹配不干净\n",
    "            text=re.findall('<p>(.*?)</p>',stringn,re.S)    #为什么有时候大写，有时候小写。。。\n",
    "#        print(text)\n",
    "            contents=title+text             #这是个列表，每个元素是一串文字，而不是一个\n",
    "            path='D:/python/baogao_county/libaiwu/'\n",
    "            f=open(path+'_'+str(id)+'_'+title[0]+'.txt','w+',encoding='utf-8') \n",
    "            for content in contents:\n",
    "                f.write(content+'\\n')\n",
    "            f.close()\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        continue\n",
    "    time.sleep(0.5)           #爬取间隔数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【实例操作】礼拜五秘书网的文件整理\n",
    "import os    \n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import shutil\n",
    "path='D:/python/baogao_county/libaiwu/'\n",
    "pathn='D:/python/baogao_county/libaiwunew/'\n",
    "files= os.listdir(path)  #得到文件夹下的所有文件名称\n",
    "#print(files)  #这是一个列表\n",
    "for file in files:\n",
    "    if '政府工作报告' in file:\n",
    "        f=open(path+str(file),encoding='utf-8')\n",
    "        text=f.read()\n",
    "        text=text.replace('&nbsp;',' ')\n",
    "        text=text.replace('<b>','')\n",
    "        text=text.replace('</b>','')\n",
    "        text=text.replace('</strong>','')\n",
    "        text=text.replace('<strong>','')\n",
    "        text=text.replace('<a href=\"https://www.libaiwu.com/hwd/\">','')\n",
    "        text=text.replace('</a>','')  \n",
    "        text=text.replace('</br >','')\n",
    "        text=text.replace('<wbr />','')\n",
    "        fn=open(pathn+str(file),'w+',encoding='utf-8')\n",
    "        fn.write(text)\n",
    "        f.close()\n",
    "        fn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【实例操作】报告重命名\n",
    "#要替换，而不是另存，方便知道哪些没改成功【其实也可以存入另一个文件夹，然后原始的文件删除】\n",
    "import os    \n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import shutil\n",
    "\n",
    "path='D:/python/baogao_county/lianhehuinew/'\n",
    "files= os.listdir(path)  #得到文件夹下的所有文件名称\n",
    "#print(files)  #这是一个列表\n",
    "for file in files:\n",
    "    olddir=os.path.join(path,file)\n",
    "    filename=os.path.splitext(file)[0]\n",
    "    filetype=os.path.splitext(file)[1]\n",
    "    filen=file.replace(' ','')\n",
    "    a=filen.find('年')\n",
    "    b=filen.find('政府工作报告')\n",
    "  \n",
    "    newdir=os.path.join(path,filename[a+1:b]+filename[a-4:a]+filetype);#新的文件路径\n",
    "    try:\n",
    "        os.rename(olddir,newdir)\n",
    "    except: \n",
    "        os.remove(olddir)         #有不少重复的，将重复的删除，但这可能是将所有异常全部删除了？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baogao='_3238_2013年济南市历城区人民政府工作报告'\n",
    "a=baogao.find('年')\n",
    "b=baogao.find('人民政府工作报告')\n",
    "print(a,b)\n",
    "print(baogao[a+1:b]+baogao[a-4:a])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【实际应用案例】：政府工作报告的爬取,知县网\n",
    "from urllib import request, parse\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError,URLError\n",
    "from bs4 import BeautifulSoup \n",
    "import time  \n",
    "import re\n",
    "import requests\n",
    "head={}  #字典结构\n",
    "head['User-Agent']='Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'\n",
    "#读取id\n",
    "path='D:/python/baogao_county/zhixian/' \n",
    "for id in range(9888,10110): \n",
    "    url='http://www.ahmhxc.com/gongzuobaogao/'+str(id)+'.html'\n",
    "#        http://www.ahmhxc.com/gongzuobaogao/10101.html       #模仿此公式计算而来\n",
    "    print(url)\n",
    "    req=requests.get(url,headers=head)\n",
    "    if req.status_code==200:\n",
    "        resp=request.urlopen(url)\n",
    "        html=resp.read()\n",
    "        string=html.decode(encoding='gb2312',errors='ignore')  \n",
    "#        print(string)\n",
    "        title=re.findall('<div class=\"title\"><h1>(.*?)</h1></div>',string) \n",
    "#                  <div class=\"title\"><h1>（河南省）2018年柘城县人民政府工作报告（全文）</h1></div>\n",
    "#        title=re.findall('<title>(.*?) _知县网---最有深度、最有态度、最接地气的县域大数据门户</title>',string)  \n",
    "#    <title>（河南省）2018年柘城县人民政府工作报告（全文）_知县网---最有深度、最有态度、最接地气的县域大数据门户</title>\n",
    "        print(title)\n",
    "        text0=re.findall(' <script src=\"/d/js/acmsd/thea8.js\"></script>(.*?)<div class=\"pagebreak\">',string,re.S)    \n",
    "#                         <p style=\"text-align: justify;\"><span style=\"font-size:14px;\">\n",
    "#           print(text)\n",
    "        contents=title+text0            #这是个列表，每个元素是一串文字，而不是一个\n",
    "        for p in range(2,10):\n",
    "            url='http://www.ahmhxc.com/gongzuobaogao/'+str(id)+'_'+str(p)+'.html'\n",
    "#                http://www.ahmhxc.com/gongzuobaogao/10064_3.html       #模仿此公式计算而来\n",
    "            req=requests.get(url,headers=head)\n",
    "            if req.status_code==200:\n",
    "                resp=request.urlopen(url)\n",
    "                html=resp.read()\n",
    "                string=html.decode(encoding='gb2312',errors='ignore') \n",
    "                text=re.findall(' <script src=\"/d/js/acmsd/thea8.js\"></script>(.*?)<div class=\"pagebreak\">',string,re.S) \n",
    "                contents=contents+text\n",
    "            else:\n",
    "                continue\n",
    "        f=open(path+'_'+str(id)+'_'+title[0]+'.txt','w+',encoding='gb2312') \n",
    "        for content in contents:\n",
    "\n",
    "            f.write(content+'\\n')\n",
    "        f.close()\n",
    "    else:\n",
    "        continue\n",
    "    time.sleep(0.5)           #爬取间隔数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【实例操作】知县网的文件整理\n",
    "import os    \n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import shutil\n",
    "path='D:/python/baogao_county/zhixian/'\n",
    "pathn='D:/python/baogao_county/zhixiannew/'\n",
    "files= os.listdir(path)  #得到文件夹下的所有文件名称\n",
    "#print(files)  #这是一个列表\n",
    "for file in files:\n",
    "    if '政府工作报告' in file:\n",
    "        f=open(path+str(file),encoding='gb2312')\n",
    "        text=f.read()\n",
    "        text=text.replace('<!--/广告位-->','')\n",
    "        text=text.replace('<div>',\"\")\n",
    "        text=text.replace('</div>','')            \n",
    "        text=text.replace('&rdquo;','')\n",
    "        text=text.replace('&nbsp;','')\n",
    "        text=text.replace('&ldquo;','')\n",
    "        text=text.replace('</td>','')\n",
    "        text=text.replace('</tr>','')\n",
    "        text=text.replace('</table>','')\n",
    "        text=text.replace('&mdash;','')\n",
    "        text=text.replace('<br />','')\n",
    "        text=text.replace('</strong>','')\n",
    "        text=text.replace('<strong>','')\n",
    "        text=text.replace('<div class=\"TRS_Editor\">','')\n",
    "        text=text.replace('<p align=\"center\" style=\"MARGIN-BOTTOM: 0px; MARGIN-TOP: 0px\">','')\n",
    "        text=text.replace('</p>','')\n",
    "        text=text.replace('<p>','')\n",
    "        text=text.replace('<!--EpointContent-->','')\n",
    "        text=text.replace('<p style=\"TEXT-ALIGN: center\">','')\n",
    "        text=text.replace('</span>','')\n",
    "        text=text.replace('<p style=\"text-align: justify;\"><span style=\"font-size:14px;\">','')\n",
    "        text=text.replace('<div style=\"text-align: justify;\">','')\n",
    "        text=text.replace('<span style=\"font-size:14px;\">','')\n",
    "        text=text.replace('<p style=\"text-align: justify;\">','')\n",
    "        fn=open(pathn+str(file),'w+',encoding='utf-8')\n",
    "        fn.write(text)\n",
    "        f.close()\n",
    "        fn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【实例操作】知县网报告的重命名\n",
    "#要替换，而不是另存，方便知道哪些没改成功\n",
    "import os    \n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import shutil\n",
    "\n",
    "path='D:/python/baogao_county/zhixiannew/'\n",
    "files= os.listdir(path)  #得到文件夹下的所有文件名称\n",
    "#print(files)  #这是一个列表\n",
    "i=0\n",
    "for file in files:\n",
    "    i=i+1\n",
    "    olddir=os.path.join(path,file)\n",
    "    filename=os.path.splitext(file)[0]\n",
    "    filetype=os.path.splitext(file)[1]\n",
    "    a=file.find('年')\n",
    "    b=file.find('人民政府工作报告')\n",
    "  \n",
    "    newdir=os.path.join(path,filename[a+1:b]+filename[a-4:a]+filetype);#新的文件路径\n",
    "    try:\n",
    "        os.rename(olddir,newdir)\n",
    "    except: \n",
    "        os.remove(olddir)   #有不少重复的，将重复的删除，但这可能是将所有异常全部删除了？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【实例操作】报告重命名\n",
    "#思路，分别读取三份省级、地市级以及县级的代码，然后在上述文件夹当中进行匹配\n",
    "import os    \n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import shutil\n",
    "path1='d:/python/baogao_county/省份名称.txt'\n",
    "path2='d:/python/baogao_county/城市名称.txt'\n",
    "path3='d:/python/baogao_county/县名称1.txt'\n",
    "path4='d:/python/baogao_county/县名称2.txt'\n",
    "path5='d:/python/baogao_county/县名称3.txt'\n",
    "prov=open(path1,'r',encoding='gb2312')\n",
    "prov_name=prov.read().split('\\n')\n",
    "city=open(path2,'r',encoding='gb2312')\n",
    "city_name=city.read().split('\\n')\n",
    "\n",
    "county1=open(path3,'r')\n",
    "county_name1=county1.read().split('\\n')\n",
    "county2=open(path4,'r')\n",
    "county_name2=county2.read().split('\\n')\n",
    "county3=open(path5,'r')\n",
    "county_name3=county3.read().split('\\n')\n",
    "#print(county_name3[0:10],county_name2[0:10])\n",
    "\n",
    "path='D:/python/baogao_county/rename/'\n",
    "files= os.listdir(path)  #得到文件夹下的所有文件名称\n",
    "#print(files)  #这是一个列表\n",
    "for file in files:\n",
    "    olddir=os.path.join(path,file)\n",
    "    filename=os.path.splitext(file)[0]\n",
    "    city=filename[0:-4]\n",
    "    year=filename[-4:]\n",
    "    filetype=os.path.splitext(file)[1]\n",
    "    try:\n",
    "        i=county_name3.index(city)\n",
    "        cityn=county_name2[i]\n",
    "        newdir=os.path.join(path,cityn+year+filetype);#新的文件路径\n",
    "        os.rename(olddir,newdir)\n",
    "        print(cityn)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【实际应用】国家哲学社会科学期刊数据库信息爬虫\n",
    "#首先要爬取每篇没篇文章的一个网页id\n",
    "from urllib import request, parse\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError,URLError\n",
    "from bs4 import BeautifulSoup \n",
    "import time  \n",
    "import re\n",
    "import requests\n",
    "import csv\n",
    "path='D:/python/学术期刊/'   #期次文件所在目录\n",
    "path2='D:/python/学术期刊/id/'  # 论文id保存文件夹目录\n",
    "f=open(path2+'金融研究.csv','a',encoding='utf-8',newline='')  #需要手工修改期刊名称\n",
    "journal_id='97926X'         #需要手工修改期刊id，期刊id为手工整理\n",
    "c=csv.writer(f)\n",
    "issue=open(path+'期次.csv','r',encoding='utf-8')\n",
    "issueid=issue.read()\n",
    "issueids=issueid.split('\\n')\n",
    "issueids=issueids[0:205]        #多出一行，末尾为空，搞不清楚原因，可能是上一行\\n切割导致的,第一个也有问题\n",
    "head={}  #字典结构\n",
    "head['User-Agent']='Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Mobile Safari/537.36'\n",
    "for i in range(1,217):   #到2001年1-2017年12月，204个月。\n",
    "    url=' http://www.nssd.org/journal/cn/'+journal_id+'/'+str(issueids[i])+'/'\n",
    "    print(i,url)\n",
    "    req=requests.get(url,headers=head)\n",
    "    if req.status_code==200:\n",
    "        resp=request.urlopen(url)\n",
    "        html=resp.read()\n",
    "        string=html.decode(encoding='utf-8',errors='ignore')  \n",
    "        ids=re.findall('article_detail.aspx\\?id=(.*?)\">(.*?)</a></td>',string,re.S)   #?id当中的？需要转义符处理一下        \n",
    "        for id in ids:\n",
    "            c.writerow((id[0],str(issueids[i])))\n",
    "    else:\n",
    "        continue\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【实际应用】国家哲学社会科学期刊数据库信息爬虫\n",
    "#根据上一篇爬取的网页，再进行正式爬取作者及单位信息，其他信息不要\n",
    "from urllib import request, parse\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError,URLError\n",
    "from bs4 import BeautifulSoup \n",
    "import time  \n",
    "import re\n",
    "import requests\n",
    "import csv\n",
    "path1='D:/python/学术期刊/id_new/'  #论文id读取目录\n",
    "path2='D:/python/学术期刊/paper/'   #论文信息保存目录\n",
    "f=open(path1+'经济研究.txt','r',encoding='utf-8')    #需要手工修改期刊名称\n",
    "fn=open(path2+'经济研究paper2.csv','w+',encoding='utf-8',newline='')   #需要手工修改期刊\n",
    "c=csv.writer(fn)\n",
    "c.writerow(('paper_id','title','journal','authors','units'))\n",
    "head={}  #字典结构\n",
    "head['User-Agent']='Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Mobile Safari/537.36'\n",
    "\n",
    "ids=f.read().split()\n",
    "for id in ids:\n",
    "    if id.isdigit():\n",
    "        url=' http://www.nssd.org/articles/article_detail.aspx?id='+str(id)\n",
    "#   http://www.nssd.org/articles/article_detail.aspx?id=4365341\n",
    "    else:\n",
    "        continue\n",
    "    print(url)\n",
    "    req=requests.get(url,headers=head)\n",
    "    if req.status_code==200:\n",
    "        resp=request.urlopen(url)\n",
    "        html=resp.read()\n",
    "        string=html.decode(encoding='utf-8',errors='ignore')  \n",
    "        #string= html.decode('utf-8').encode(type)\n",
    "        #print(string)\n",
    "        title=re.findall('<title>(.*?)-国家哲学社会科学学术期刊数据库</title>',string,re.S) \n",
    "        if title==[]:\n",
    "            title=['no_title']\n",
    "        journal=re.findall('.*【期&#12288;&#12288;刊】.*title=\\'\\'>(.*?)</a>  <i>.*',string,re.S)\n",
    "        if journal==[]:\n",
    "            journal=['no_journal']\n",
    "        authors0=re.findall('【作&#12288;&#12288;者】(.*?)【作者单位】',string,re.S)\n",
    "        if authors0==[]:\n",
    "            authors=['no_author']\n",
    "        else:\n",
    "            authors=re.findall('title=\\'\\'>(.*?)</a>',authors0[0],re.S)\n",
    "        if authors==[]:\n",
    "            authors=['no_author']\n",
    "        units0=re.findall('【作者单位】(.*?)【期&#12288;&#12288;刊】',string,re.S)\n",
    "        if units0==[]:\n",
    "            units=['no_unit']\n",
    "        else:\n",
    "            units=re.findall('title=\\'\\'>(.*?)</a>',units0[0],re.S)\n",
    "        if units==[]:\n",
    "            units=['no_unit']\n",
    "        print(title)\n",
    "        print(journal)\n",
    "        print(authors)\n",
    "        print(units)\n",
    "        c.writerow((id,title[0],journal[0],\";\".join(authors),\";\".join(units)))\n",
    "    else:\n",
    "        continue\n",
    "f.close() \n",
    "fn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#中国知网数据爬虫\n",
    "# -*-coding:utf-8-*-\n",
    "import pymysql\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "\n",
    "class Cnki:\n",
    "    agents = [\n",
    "        \"Mozilla/5.0 (Linux; U; Android 2.3.6; en-us; Nexus S Build/GRK39F) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1\",\n",
    "        \"Avant Browser/1.2.789rel1 (http://www.avantbrowser.com)\",\n",
    "        \"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.5 (KHTML, like Gecko) Chrome/4.0.249.0 Safari/532.5\",\n",
    "        \"Mozilla/5.0 (Windows; U; Windows NT 5.2; en-US) AppleWebKit/532.9 (KHTML, like Gecko) Chrome/5.0.310.0 Safari/532.9\",\n",
    "        \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/534.7 (KHTML, like Gecko) Chrome/7.0.514.0 Safari/534.7\",\n",
    "        \"Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/9.0.601.0 Safari/534.14\",\n",
    "        \"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/10.0.601.0 Safari/534.14\",\n",
    "        \"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.20 (KHTML, like Gecko) Chrome/11.0.672.2 Safari/534.20\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.27 (KHTML, like Gecko) Chrome/12.0.712.0 Safari/534.27\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.24 Safari/535.1\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.0) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/15.0.874.120 Safari/535.2\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.36 Safari/535.7\",\n",
    "        \"Mozilla/5.0 (Windows; U; Windows NT 6.0 x64; en-US; rv:1.9pre) Gecko/2008072421 Minefield/3.0.2pre\",\n",
    "    ]\n",
    "    cookie = {\n",
    "         \"Cookie\": 'Ecp_notFirstLogin=zXyitT; Ecp_ClientId=3171111123202637951; cnkiUserKey=56ef38ce-f0bf-6489-1c03-c504b7797c78; _pk_id=2a7db67d-0ddf-49a7-a737-3fbcf821bd4c.1523005065.28.1530758366.1530758366.; RsPerPage=50; __uid=722d21de2d1490a507013e4d2fed2fca; amid=1f66a89a-01ae-4ba2-b293-7adc79758a44; UM_distinctid=16712a3629191-00a248c7d224f-3c604504-e1000-16712a3629210e; ASP.NET_SessionId=2dgglsciss52obcr1rgoxrog; SID_kns=123117; SID_klogin=125144; SID_kinfo=125105; KNS_SortType=; SID_krsnew=125132; SID_kcms=124120; SID_knsdelivery=125122; Ecp_lout=1; IsLogin=; CNZZDATA3258975=cnzz_eid%3D1492256385-1517302996-http%253A%252F%252Fnvsm.cnki.net%252F%26ntime%3D1542973532; _pk_ref=%5B%22%22%2C%22%22%2C1542977641%2C%22http%3A%2F%2Fwww.cnki.net%2F%22%5D; _pk_ses=*; LID=WEEvREcwSlJHSldTTEYzVnB3ZDByTWVaZzIycTFDVS8rQ0YvVFlZZ29jcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!; Ecp_session=1; Ecp_LoginStuts=%7B%22IsAutoLogin%22%3Afalse%2C%22UserName%22%3A%22guofengsfi2%22%2C%22ShowName%22%3A%22guofengsfi2%22%2C%22UserType%22%3A%22jf%22%2C%22r%22%3A%22zXyitT%22%7D; c_m_LinID=LinID=WEEvREcwSlJHSldTTEYzVnB3ZDByTWVaZzIycTFDVS8rQ0YvVFlZZ29jcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&ot=11/25/2018 21:14:10; c_m_expire=2018-11-25 21:14:10',\n",
    "        }  # 【这里隔段时间需要自己填写cookies】\n",
    "    UA = random.choice(agents)\n",
    "    header = {'User-Agent': UA}\n",
    "    cnx = pymysql.connect(user='root', password='guo840306', host='localhost', database='cnki6',charset='utf8')  #填写数据库信息\n",
    "    cur = cnx.cursor()\n",
    "\n",
    "    def paper_info(self):\n",
    "        for i in range(1,14):  #【页数以及下面的网页需要自己填写】\n",
    "            time.sleep(random.choice([1,2,3]))\n",
    "            url = 'http://kns.cnki.net/kns/brief/brief.aspx?curpage={}&RecordsPerPage=50&QueryID=11&ID=&turnpage=1&tpagemode=L&dbPrefix=CJFQ&Fields=&DisplayMode=listmode&PageName=ASP.brief_result_aspx&isinEn=1'.format(i)\n",
    "            print(url)\n",
    "            req = requests.get(url, headers=self.header, cookies = self.cookie)\n",
    "            soup1 = BeautifulSoup(req.text, 'lxml').find('table', class_='GridTableContent').find_all('tr', bgcolor = '#ffffff')\n",
    "            soup2 = BeautifulSoup(req.text, 'lxml').find('table', class_='GridTableContent').find_all('tr', bgcolor = '#f6f7fb')\n",
    "            for s1 in soup1:\n",
    "                paper_name = ''\n",
    "                author = ''\n",
    "                author_unit = ''\n",
    "                mag_name = ''\n",
    "                year_period = ''\n",
    "                cited = 0\n",
    "                download = 0\n",
    "                gjc = ''\n",
    "                zhaiyao = ''\n",
    "                fund = ''\n",
    "                class_num = ''\n",
    "                page_num = 0\n",
    "                page_range = ''\n",
    "\n",
    "                td = s1.find_all('td')\n",
    "                paper_name =td[1].get_text().strip('\\n').strip()\n",
    "                author = td[2].get_text().strip('\\n').strip()\n",
    "                mag_name = td[3].get_text().strip('\\n').strip()\n",
    "                year_period = td[4].get_text().strip('\\n').strip()\n",
    "                cited = td[5].get_text().strip('\\n').strip()\n",
    "                if cited ==  '':\n",
    "                    cited = 0\n",
    "                download = td[6].get_text().strip('\\n').strip()\n",
    "                if download == '':\n",
    "                    download = 0\n",
    "\n",
    "                href = 'http://kns.cnki.net/KCMS' + td[1].find('a')['href'].replace('/kns', '').replace(' ','')\n",
    "                req2 = requests.get(href, headers=self.header, cookies = self.cookie)\n",
    "                time.sleep(0.5)\n",
    "                soup3 = BeautifulSoup(req2.text, 'lxml').find('div', class_='wxBaseinfo')\n",
    "\n",
    "                if soup3.find('span', id='ChDivSummary') != None:\n",
    "                    zhaiyao = soup3.find('span', id='ChDivSummary').get_text()\n",
    "\n",
    "                p = soup3.find_all('p')\n",
    "                for p1 in p:\n",
    "                    if p1.find('label', id='catalog_KEYWORD') != None:\n",
    "                        gjc = p1.get_text().replace('\\n', '').replace('                  ','').strip()\n",
    "                        print(gjc)\n",
    "                    if p1.find('label', id='catalog_FUND') != None:\n",
    "                        fund = p1.get_text().replace('\\n', '').replace('                  ', '').strip()\n",
    "                        print(fund)\n",
    "                    if p1.find('label', id='catalog_ZTCLS') != None:\n",
    "                        class_num = p1.get_text().replace('\\n', '').strip()\n",
    "                        print(class_num)\n",
    "\n",
    "                if BeautifulSoup(req2.text, 'lxml').find('div', class_='orgn')!= None:\n",
    "                    author_unit_list = BeautifulSoup(req2.text, 'lxml').find('div', class_='orgn').find_all('span')\n",
    "                    for s in author_unit_list:\n",
    "                        author_unit = author_unit + '  ' + s.get_text()\n",
    "\n",
    "                if BeautifulSoup(req2.text, 'lxml').find('div', class_='dllink-down')!= None:\n",
    "                    page_list = BeautifulSoup(req2.text, 'lxml').find('div', class_='dllink-down').find('div', class_='total').find_all('span')\n",
    "                    for p in page_list:\n",
    "                        if p.find('label').get_text() == '页码：':\n",
    "                            page_range = p.find('b').get_text()\n",
    "                        if p.find('label').get_text() == '页数：':\n",
    "                            page_num = p.find('b').get_text()\n",
    "\n",
    "                print(paper_name + '  ' + author + '    ' + mag_name + '    ' + year_period)\n",
    "\n",
    "                sql = 'INSERT INTO cnki  (`paper_name`,`author`,`author_unit`, `mag_name`, `year_period` ,`cited`,\\\n",
    "                  `download`, `gjc`,  `zhaiyao`, `fund`, `class_num`, `page_num`, `page_range`) \\\n",
    "                    VALUES (%(paper_name)s, %(author)s, %(author_unit)s, %(mag_name)s, %(year_period)s,\\\n",
    "                      %(cited)s, %(download)s, %(gjc)s, %(zhaiyao)s, %(fund)s, %(class_num)s, %(page_num)s, %(page_range)s)'\n",
    "                value = {\n",
    "                    'paper_name':paper_name,\n",
    "                    'author':author,\n",
    "                    'author_unit':author_unit,\n",
    "                    'mag_name':mag_name,\n",
    "                    'year_period':year_period,\n",
    "                    'cited':cited,\n",
    "                    'download':download,\n",
    "                    'gjc':gjc,\n",
    "                    'zhaiyao':zhaiyao,\n",
    "                    'fund':fund,\n",
    "                    'class_num':class_num,\n",
    "                    'page_num':page_num,\n",
    "                    'page_range':page_range\n",
    "                }\n",
    "                self.cur.execute(sql, value)\n",
    "                self.cnx.commit()\n",
    "\n",
    "            for s2 in soup2:\n",
    "                paper_name = ''\n",
    "                author = ''\n",
    "                author_unit = ''\n",
    "                mag_name = ''\n",
    "                year_period = ''\n",
    "                cited = 0\n",
    "                download = 0\n",
    "                gjc = ''\n",
    "                zhaiyao = ''\n",
    "                fund = ''\n",
    "                class_num = ''\n",
    "                page_num = 0\n",
    "                page_range = ''\n",
    "\n",
    "                td = s2.find_all('td')\n",
    "                paper_name =td[1].get_text().strip('\\n').strip()\n",
    "                author = td[2].get_text().strip('\\n').strip()\n",
    "                mag_name = td[3].get_text().strip('\\n').strip()\n",
    "                year_period = td[4].get_text().strip('\\n').strip()\n",
    "                cited = td[5].get_text().strip('\\n').strip()\n",
    "                if cited ==  '':\n",
    "                    cited = 0\n",
    "                download = td[6].get_text().strip('\\n').strip()\n",
    "                if download == '':\n",
    "                    download = 0\n",
    "                href1 = 'http://kns.cnki.net/KCMS' + td[1].find('a')['href'].replace('/kns', '').replace(' ','')\n",
    "\n",
    "                req3 = requests.get(href1, headers=self.header, cookies = self.cookie)\n",
    "                time.sleep(0.5)\n",
    "                soup4 = BeautifulSoup(req3.text, 'lxml').find('div', class_='wxBaseinfo')\n",
    "\n",
    "                if soup4.find('span', id = 'ChDivSummary') != None:\n",
    "                    zhaiyao = soup4.find('span', id = 'ChDivSummary').get_text()\n",
    "\n",
    "                pp = soup4.find_all('p')\n",
    "                for p2 in pp:\n",
    "                    if p2.find('label', id='catalog_KEYWORD') != None:\n",
    "                        gjc = p2.get_text().replace('\\n', '').replace('                  ', '').strip()\n",
    "                        print(gjc)\n",
    "                    if p2.find('label', id='catalog_FUND') != None:\n",
    "                        fund = p2.get_text().replace('\\n', '').replace('                  ', '').strip()\n",
    "                        print(fund)\n",
    "                    if p2.find('label', id='catalog_ZTCLS') != None:\n",
    "                        class_num = p2.get_text().replace('\\n', '').strip()\n",
    "                        print(class_num)\n",
    "\n",
    "                if BeautifulSoup(req3.text, 'lxml').find('div', class_='orgn') != None:\n",
    "                    author_unit_list = BeautifulSoup(req3.text, 'lxml').find('div', class_='orgn').find_all('span')\n",
    "                    for s in author_unit_list:\n",
    "                        author_unit = author_unit + '  ' + s.get_text()\n",
    "\n",
    "                if BeautifulSoup(req3.text, 'lxml').find('div', class_='dllink-down') != None:\n",
    "                    page_list2 = BeautifulSoup(req3.text, 'lxml').find('div', class_='dllink-down').find('div',class_='total').find_all('span')\n",
    "                    for p in page_list2:\n",
    "                        if p.find('label').get_text() == '页码：':\n",
    "                            page_range = p.find('b').get_text()\n",
    "                        if p.find('label').get_text() == '页数：':\n",
    "                            page_num = p.find('b').get_text()\n",
    "                print(paper_name + '  ' + author + '    ' + mag_name + '    ' + year_period)\n",
    "\n",
    "                sql = 'INSERT INTO cnki (`paper_name`,`author`,`author_unit`, `mag_name`, `year_period` ,`cited`,\\\n",
    "                      `download`, `gjc`, `zhaiyao`, `fund`, `class_num`, `page_num`, `page_range`) \\\n",
    "                       VALUES (%(paper_name)s, %(author)s, %(author_unit)s, %(mag_name)s, %(year_period)s,\\\n",
    "                   %(cited)s, %(download)s, %(gjc)s, %(zhaiyao)s, %(fund)s, %(class_num)s, %(page_num)s, %(page_range)s)'\n",
    "                value = {\n",
    "                    'paper_name': paper_name,\n",
    "                    'author': author,\n",
    "                    'author_unit': author_unit,\n",
    "                    'mag_name': mag_name,\n",
    "                    'year_period': year_period,\n",
    "                    'cited': cited,\n",
    "                    'download': download,\n",
    "                    'gjc': gjc,\n",
    "                    'zhaiyao': zhaiyao,\n",
    "                    'fund': fund,\n",
    "                    'class_num': class_num,\n",
    "                    'page_num': page_num,\n",
    "                    'page_range': page_range\n",
    "                }\n",
    "                self.cur.execute(sql, value)\n",
    "                self.cnx.commit()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cnki = Cnki()  # 调用Cnki类，创建实例\n",
    "    cnki.paper_info()\n",
    "    print(\"**************爬取结束******************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#中国哲学社会期刊数据库网站爬取\n",
    "import pymysql\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "import requests\n",
    "\n",
    "class Nssd:\n",
    "\n",
    "    agents = [\n",
    "        \"Mozilla/5.0 (Linux; U; Android 2.3.6; en-us; Nexus S Build/GRK39F) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1\",\n",
    "        \"Avant Browser/1.2.789rel1 (http://www.avantbrowser.com)\",\n",
    "        \"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.5 (KHTML, like Gecko) Chrome/4.0.249.0 Safari/532.5\",\n",
    "        \"Mozilla/5.0 (Windows; U; Windows NT 5.2; en-US) AppleWebKit/532.9 (KHTML, like Gecko) Chrome/5.0.310.0 Safari/532.9\",\n",
    "        \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/534.7 (KHTML, like Gecko) Chrome/7.0.514.0 Safari/534.7\",\n",
    "        \"Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/9.0.601.0 Safari/534.14\",\n",
    "        \"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/10.0.601.0 Safari/534.14\",\n",
    "        \"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.20 (KHTML, like Gecko) Chrome/11.0.672.2 Safari/534.20\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.27 (KHTML, like Gecko) Chrome/12.0.712.0 Safari/534.27\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.24 Safari/535.1\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.0) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/15.0.874.120 Safari/535.2\",\n",
    "    ]\n",
    "\n",
    "    cnx = pymysql.connect(user='root', password='guo840306', host='localhost', database='nssd',charset='utf8mb4')\n",
    "    cur = cnx.cursor()\n",
    "\n",
    "\n",
    "    def get_info(self, journal):\n",
    "\n",
    "        UA = random.choice(self.agents)\n",
    "        header = {'User-Agent': UA}\n",
    "\n",
    "        for year in range(2001, 2018):\n",
    "            url = 'http://www.nssd.org/journal/cn/%sB/%s01/?lis=1' % (str(journal), str(year))\n",
    "            print(url)\n",
    "            response = requests.get(url, headers = header)\n",
    "            time.sleep(0.5)\n",
    "            #print(response.text)\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "            li_list = soup.find('div', id='numlist').find_all('li')\n",
    "            for li in range(0,len(li_list)):  # 期数len(li_list)为期总数\n",
    "                href = 'http://www.nssd.org/' + li_list[li].find('a')['href']\n",
    "                response2 = requests.get(href, headers = header)\n",
    "                time.sleep(0.5)\n",
    "                soup2 = BeautifulSoup(response2.text, 'lxml')\n",
    "                tr_list = soup2.find('table', class_='t_list').find_all('tr')\n",
    "                for tr in tr_list:\n",
    "                    if tr.find('td'):\n",
    "                        paper_name = ''\n",
    "                        author = ''\n",
    "                        author_unit = ''\n",
    "                        mag_name = ''\n",
    "                        detail_url =  'http://www.nssd.org/' +  tr.find('td').find('a')['href']\n",
    "                        response3 = requests.get(detail_url, headers = header)\n",
    "                        time.sleep(0.5)\n",
    "                        soup3 = BeautifulSoup(response3.text, 'lxml')\n",
    "                        detail = soup3.find('div', class_='summary')\n",
    "                        paper_name = detail.find('h1').get_text()\n",
    "                        print(paper_name)\n",
    "                        p_list = detail.find('div', class_='article_detail').find_all('p')\n",
    "                        for p in p_list:\n",
    "                            if p.find('strong').get_text() == '【作　　者】':\n",
    "                                a_list = p.find_all('a')\n",
    "                                for a in a_list:\n",
    "                                    author = author + a.get_text() + '  '\n",
    "                            if p.find('strong').get_text() == '【作者单位】':\n",
    "                                a_list = p.find_all('a')\n",
    "                                for a in a_list:\n",
    "                                    author_unit = author_unit + a.get_text() + '  '\n",
    "                            if p.find('strong').get_text() == '【期　　刊】':\n",
    "                                mag_name = p.get_text().split(' ')[0] + ' ' + p.get_text().split(' ')[-1]\n",
    "                                mag_name = mag_name.replace('【期　　刊】', '')\n",
    "                        print(author_unit)\n",
    "                        print(author)\n",
    "                        print(mag_name)\n",
    "\n",
    "                        sql = 'INSERT INTO nssd (`paper_name`, `author`, `author_unit`,  `mag_name`) VALUES ( %(paper_name)s, %(author)s, %(author_unit)s, %(mag_name)s)'\n",
    "                        value = {\n",
    "                            'paper_name': paper_name,\n",
    "                            'author': author,\n",
    "                            'author_unit': author_unit,\n",
    "                            'mag_name': mag_name\n",
    "                        }\n",
    "                        self.cur.execute(sql, value)\n",
    "                        self.cnx.commit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nssd = Nssd()\n",
    "    journal = [98338]           #期刊代码\n",
    "    for jour in journal:\n",
    "        nssd.get_info(jour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
